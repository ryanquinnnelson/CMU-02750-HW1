{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. IWAL algorithm implementation (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this question is to implement Importance Weighted Active Learning (IWAL) algorithm. For this question, you will not use modAL, but instead will implement IWAL routine from scratch using scikit-learn, NumPy and native Python. \n",
    "\n",
    "In this question, we will use a simple synthetic dataset for a binary classification problem. Each data point has only 2 features. The dataset is provided in 2 files -- “data_iwal.npy”, which contains features and “labels_iwal.npy”, which contains labels. \n",
    "\n",
    "For simplicity, you will implement bootstrapping rejection sampling subroutine with logistic regression and hinge loss.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝐂𝐨𝐦𝐩𝐥𝐞𝐭𝐞 𝐭𝐡𝐞 𝐜𝐨𝐝𝐞 𝐮𝐧𝐝𝐞𝐫 ###𝐓𝐎 𝐃𝐎 𝐢𝐧 𝐞𝐚𝐜𝐡 𝐜𝐞𝐥𝐥 𝐚𝐧𝐝 𝐩𝐫𝐨𝐝𝐮𝐜𝐞 𝐭𝐡𝐞 𝐫𝐞𝐪𝐮𝐢𝐫𝐞𝐝 𝐩𝐥𝐨𝐭𝐬.  Feel free to define any helper functions as you see fit. You may import and use any modules in scikit-learn and NumPy to help with your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import necessary modules. Feel free to add something else here if you need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandomize sttream\\nremove from stream once selected\\nhinge loss normalization?\\n\\nH is local to subroutine\\nretrain committee each time\\ntrain outer models only after history contains both labels\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo\n",
    "'''\n",
    "randomize sttream\n",
    "remove from stream once selected\n",
    "hinge loss normalization?\n",
    "\n",
    "H is local to subroutine\n",
    "retrain committee each time\n",
    "train outer models only after history contains both labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [[3.20896197 1.01262309]]\n",
      "y_true: [1]\n",
      "y_pred1: [1.70962584]\n",
      "y_pred2: [3.07829593]\n",
      "hinge loss model 1: 2.709625837320441\n",
      "hinge loss model 2: 4.078295932657506\n",
      "difference: 1.3686700953370647\n",
      "Examples in which loss difference is not [0,1]\n",
      "1.7852096113335079\n",
      "2.045987413240476\n",
      "1.966139981501183\n",
      "2.174981066580435\n",
      "2.216352911788471\n",
      "1.3686700953370647\n",
      "1.6880342091713212\n",
      "1.7082533609172144\n",
      "2.2848864788093897\n",
      "1.5851951862707603\n",
      "1.8580464490850055\n",
      "1.2274217363809252\n",
      "2.0695993714323286\n",
      "1.741139162674572\n",
      "2.494994241341735\n",
      "2.210688011907143\n",
      "1.299257672366021\n",
      "2.2442666039446095\n",
      "1.5145777251306862\n",
      "2.012136945219641\n",
      "2.001749387841971\n",
      "1.5403664098317225\n",
      "1.5410507454761544\n",
      "1.7530362270586086\n",
      "2.120314720037692\n",
      "2.0253044283365487\n",
      "1.2411720638455925\n",
      "1.7340626704823583\n",
      "1.4428110328885277\n",
      "1.5541191155186018\n",
      "2.3353626021830998\n",
      "1.2975552240440509\n",
      "2.107914683252658\n",
      "1.6766797550237666\n",
      "1.9132584894457483\n",
      "2.027967877403121\n",
      "1.2699711252204673\n",
      "1.9741986920759262\n",
      "1.8196537361898755\n",
      "1.9515136098541706\n",
      "2.2170422769425535\n",
      "2.0538838817891145\n",
      "1.2239778600389624\n",
      "2.276802086578539\n",
      "1.7305586172830365\n",
      "2.2790715869191756\n",
      "1.8872704490708538\n",
      "1.9070039124794393\n",
      "2.3253670348191653\n",
      "2.1579984902924183\n",
      "2.4495510389499944\n",
      "2.1287733949382974\n",
      "1.7907346428715494\n",
      "1.834204562491375\n",
      "1.5800270728624883\n",
      "2.05344794120935\n",
      "2.429040533669089\n",
      "2.438058297561239\n",
      "1.9253333200135625\n",
      "1.5699609186963817\n",
      "1.538274218045803\n",
      "2.3474738279252723\n",
      "2.404095601613099\n",
      "1.6482104949366216\n",
      "2.502585671566088\n"
     ]
    }
   ],
   "source": [
    "# example of hinge_loss normalization issue\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hinge_loss, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def select_n_random(X,y,n):\n",
    "    idx = np.random.choice(X.shape[0], n, replace=True)\n",
    "    return X[idx],y[idx]\n",
    "\n",
    "\n",
    "# load data\n",
    "X = np.load(\"data/q2/data_iwal.npy\")\n",
    "y = np.load(\"data/q2/labels_iwal.npy\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train 2 models\n",
    "X_initial,y_initial = select_n_random(X_train,y_train,10)\n",
    "lr1 = LogisticRegression().fit(X_initial,y_initial)\n",
    "\n",
    "X_initial,y_initial = select_n_random(X_train,y_train,10)\n",
    "lr2 = LogisticRegression().fit(X_initial,y_initial)\n",
    "\n",
    "\n",
    "# sample\n",
    "x1 = X_train[11].reshape(1,2)\n",
    "y_true = y_train[11].reshape(1,)\n",
    "\n",
    "# calculate hinge_loss\n",
    "y1_pred = lr1.decision_function(x1)\n",
    "y2_pred = lr2.decision_function(x1)\n",
    "\n",
    "hl1 = hinge_loss(y_true,y1_pred,labels=[0,1])\n",
    "hl2 = hinge_loss(y_true,y2_pred,labels=[0,1])\n",
    "diff = abs(hl1 - hl2)\n",
    "\n",
    "print('x1:',x1)\n",
    "print('y_true:',y_true)\n",
    "print('y_pred1:', y1_pred)\n",
    "print('y_pred2:', y2_pred)\n",
    "print('hinge loss model 1:', hl1) # not between 0 and 1\n",
    "print('hinge loss model 2:', hl2)\n",
    "print('difference:',diff)\n",
    "\n",
    "# doesn't always result in difference between [0,1]\n",
    "print('Examples in which loss difference is not [0,1]')\n",
    "for t in range(X_train.shape[0]):\n",
    "    x1 = X_train[t].reshape(1,2)\n",
    "    y_true = y_train[t].reshape(1,)\n",
    "\n",
    "    # calculate hinge_loss\n",
    "    y1_pred = lr1.decision_function(x1)\n",
    "    y2_pred = lr2.decision_function(x1)\n",
    "\n",
    "    hl1 = hinge_loss(y_true,y1_pred,labels=[0,1])\n",
    "    hl2 = hinge_loss(y_true,y2_pred,labels=[0,1])\n",
    "    diff = abs(hl1 - hl2)\n",
    "    if diff < 0.0 or diff > 1.0:\n",
    "        print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hinge_loss, log_loss\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the data and split it into train and test datasets. Train will be used to train our classification model and test will be used to validate the performance, monitor overfitting and compare the results of the model trained with Active Learning with the ones of the model trained from scratch. We set aside 1/3 of the dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"data/q2/data_iwal.npy\")\n",
    "y = np.load(\"data/q2/labels_iwal.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 2)\n",
      "(134,)\n",
      "(66, 2)\n",
      "(66,)\n",
      "[[2.59193175 1.14706863]\n",
      " [1.7756532  1.15670278]]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_training,y_training = X_train[:10],y_train[:10]\n",
    "# lr = LogisticRegression().fit(X_training,y_training)\n",
    "\n",
    "# x_t = X_train[11].reshape(1,2)\n",
    "# y_t = y_train[11].reshape(1,)\n",
    "\n",
    "\n",
    "# y_pred = lr.predict_proba(x_t)\n",
    "# loss = log_loss(y_t, y_pred,labels=[0,1])\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1\n",
    "Type your answers for the theoretical questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the idea behind IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**\n",
    "Corrects for sample bias using weighting.\n",
    "\n",
    "\n",
    "2. What are the assumptions made for the IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**\n",
    "\n",
    "3. What are the pros and cons of IWAL algorithm?\n",
    "\n",
    "**Your answer goes here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Implement IWAL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will implement a function that performs a single query of Algorithm 1 IWAL (subroutine rejection-sampling) from the paper. Below is the function description that you can follow in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Equations\n",
    "\n",
    "$h_t = \\underset{h \\in H}{argmin}\\sum_{(x,y,c)\\in S_t}c \\cdot l(h(x),y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented in custom Python package iwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Implement bootstrapping rejection sampling subroutine\n",
    "In this part you will implement bootstrapping rejection sampling subroutine from the paper, section 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Equations\n",
    "$p_t = p_{min}+(1-p_{min})[\\underset{y;h_i,h_j \\in H}{max}L(h_i(x),y)-L(h_j(x),y)]$\n",
    "\n",
    "where $p_{min}$ is a lower bound on the sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented in custom Python package iwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Organize all implemented parts into a single pipeline\n",
    "You implemented all parts of the IWAL algorithm with bootstrap rejection sampling. Now organize it into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.42 s, sys: 42.7 ms, total: 9.46 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#?? remove from data after stream? assume data in stream is randomized enough?\n",
    "import copy\n",
    "from packages.iwal import iwal  # my custom package\n",
    "\n",
    "# fill hypothesis_space\n",
    "H = []\n",
    "num_h = 10\n",
    "for i in range(num_h):\n",
    "    lr = LogisticRegression()\n",
    "    H.append(lr)\n",
    "\n",
    "# additional arguments\n",
    "bootstrap_size = 10\n",
    "labels = [0,1]\n",
    "p_min = 0.1  \n",
    "history = {'X': [],'y': [],'c': [],'Q': []}\n",
    "selected = []\n",
    "rejection_threshold = 'bootstrap'\n",
    "\n",
    "\n",
    "# copy data for use with this section\n",
    "X_train_iwal = copy.deepcopy(X_train)\n",
    "y_train_iwal = copy.deepcopy(y_train)\n",
    "\n",
    "# Perform queries and record loss\n",
    "losses = []\n",
    "n_query = 60 # first bootstrap_size queries are bootstrapping process and do not produce h_t\n",
    "for t in range(n_query):\n",
    "    \n",
    "    # select sample \n",
    "#     idx = np.random.choice(X_train_iwal.shape[0], 1, replace=False)\n",
    "    x_t = X_train_iwal[t].reshape(1,2)\n",
    "    y_t = y_train_iwal[t].reshape(1,)\n",
    "    \n",
    "#     # remove sample from stream\n",
    "#     X_train_iwal, y_train_iwal = np.delete(X_train_iwal, idx, axis=0), np.delete(y_train_iwal, idx, axis=0)\n",
    "\n",
    "    # select optimal hypothesis\n",
    "    h_t = iwal.iwal_query(x_t,y_t,H,history,selected,labels,rejection_threshold,bootstrap_size,p_min)\n",
    "    \n",
    "    # calculate loss only for fitted models\n",
    "    if h_t:\n",
    "#         print('selected model is',h_t.coef_, h_t.intercept_,'\\n')\n",
    "\n",
    "        # calculate loss and store for later\n",
    "        loss_t = log_loss(y_test, h_t.predict_proba(X_test)) \n",
    "        losses.append(loss_t)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in range(n_query):\n",
    "#     print('X:', history['X'][t],'y:',history['y'][t],'c:',history['c'][t],'Q:',history['Q'][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffeb782ccc0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.5 Compare results of Active Learning vs No Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you need to create object of the same scikit learning class and train it on randomly selected subset of data points and compare results of 2 classifiers. Comment on your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Learning loss: 0.2230795479087453\n",
      "Offline loss (no active learning): 0.12834812664522305\n"
     ]
    }
   ],
   "source": [
    "# Compare to no Active Learning setting\n",
    "# ?? train on same data set as AL?\n",
    "\n",
    "# train model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train[:bootstrap_size], y_train[:bootstrap_size])\n",
    "offline_loss = log_loss(y_test, lr.predict_proba(X_test)) \n",
    "\n",
    "# compare losses\n",
    "print('Active Learning loss:', losses[-1])\n",
    "print('Offline loss (no active learning):',offline_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on observations\n",
    "It appears that offline loss is less than active learning loss for a small sample size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
